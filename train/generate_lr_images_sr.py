# ========== è¶…åˆ†è¾¨ç‡æ•°æ®é¢„å¤„ç†è„šæœ¬ (RGBå½©è‰², 4xåˆ†è¾¨ç‡æå‡) ==========
import os
import argparse
from PIL import Image
import numpy as np
from tqdm import tqdm
import json
import time

def create_lr_hr_pairs(hr_image_path, lr_output_path, scale_factor=4):
    """
    ä»é«˜åˆ†è¾¨ç‡å›¾åƒåˆ›å»ºä½åˆ†è¾¨ç‡-é«˜åˆ†è¾¨ç‡å›¾åƒå¯¹
    
    Args:
        hr_image_path: é«˜åˆ†è¾¨ç‡å›¾åƒè·¯å¾„
        lr_output_path: ä½åˆ†è¾¨ç‡å›¾åƒè¾“å‡ºè·¯å¾„
        scale_factor: é™è´¨å€æ•°
    
    Returns:
        success: æ˜¯å¦æˆåŠŸå¤„ç†
    """
    try:
        # åŠ è½½é«˜åˆ†è¾¨ç‡å›¾åƒ (RGBå½©è‰²)
        hr_img = Image.open(hr_image_path).convert('RGB')
        
        # é«˜åˆ†è¾¨ç‡ç›®æ ‡å°ºå¯¸ (ç”¨äºè®­ç»ƒçš„ground truth)
        hr_target_size = 1024  # 1024x1024
        
        # ä½åˆ†è¾¨ç‡è¾“å…¥å°ºå¯¸ (æ¨¡å‹è¾“å…¥)
        lr_input_size = hr_target_size // scale_factor  # 256x256
        
        # 1. è°ƒæ•´é«˜åˆ†è¾¨ç‡å›¾åƒåˆ°ç›®æ ‡å°ºå¯¸
        hr_resized = hr_img.resize((hr_target_size, hr_target_size), Image.LANCZOS)
        
        # 2. åˆ›å»ºä½åˆ†è¾¨ç‡ç‰ˆæœ¬ (é€šè¿‡ä¸‹é‡‡æ ·)
        lr_downsampled = hr_resized.resize((lr_input_size, lr_input_size), Image.LANCZOS)
        
        # 3. å°†ä½åˆ†è¾¨ç‡å›¾åƒä¸Šé‡‡æ ·å›é«˜åˆ†è¾¨ç‡å°ºå¯¸ (è¿™æ˜¯æ¨¡å‹çš„è¾“å…¥)
        lr_upsampled = lr_downsampled.resize((hr_target_size, hr_target_size), Image.LANCZOS)
        
        # 4. ä½†æ˜¯å®é™…æ¨¡å‹è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸå§‹çš„ä½åˆ†è¾¨ç‡ä½œä¸ºè¾“å…¥
        # æ‰€ä»¥æˆ‘ä»¬ä¿å­˜çš„æ˜¯256x256çš„ä½åˆ†è¾¨ç‡å›¾åƒ
        lr_upsampled_for_training = lr_downsampled  # 256x256ä½œä¸ºæ¨¡å‹è¾“å…¥
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(lr_output_path), exist_ok=True)
        
        # ä¿å­˜ä½åˆ†è¾¨ç‡å›¾åƒ (256x256, ä½œä¸ºæ¨¡å‹è¾“å…¥)
        lr_upsampled_for_training.save(lr_output_path)
        
        return True, {
            'hr_size': hr_resized.size,
            'lr_size': lr_upsampled_for_training.size,
            'scale_factor': scale_factor
        }
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥ {hr_image_path}: {e}")
        return False, None

def process_dataset(hr_root, lr_root, scales, max_images_per_category=None):
    """
    å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œåˆ›å»ºè¶…åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®
    
    Args:
        hr_root: é«˜åˆ†è¾¨ç‡å›¾åƒæ ¹ç›®å½•
        lr_root: ä½åˆ†è¾¨ç‡å›¾åƒè¾“å‡ºæ ¹ç›®å½•
        scales: é™è´¨å€æ•°åˆ—è¡¨
        max_images_per_category: æ¯ä¸ªç±»åˆ«çš„æœ€å¤§å›¾åƒæ•°é‡
    """
    print("=" * 70)
    print("ğŸš€ è¶…åˆ†è¾¨ç‡æ•°æ®é¢„å¤„ç† (RGBå½©è‰²å›¾åƒ)")
    print("=" * 70)
    
    if not os.path.exists(hr_root):
        print(f"âŒ é«˜åˆ†è¾¨ç‡æ•°æ®ç›®å½•ä¸å­˜åœ¨: {hr_root}")
        return
    
    # æ”¯æŒçš„å›¾åƒæ ¼å¼
    supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
    
    processing_stats = {
        'start_time': time.time(),
        'scales': scales,
        'categories': {},
        'total_processed': 0,
        'total_failed': 0
    }
    
    for scale in scales:
        print(f"\nğŸ“Š å¤„ç† {scale}x é™è´¨...")
        
        lr_scale_root = os.path.join(lr_root, f'scale_{scale}x')
        os.makedirs(lr_scale_root, exist_ok=True)
        
        scale_stats = {
            'processed': 0,
            'failed': 0,
            'categories': {}
        }
        
        # éå†æ‰€æœ‰ç±»åˆ«ç›®å½•
        categories = [d for d in os.listdir(hr_root) if os.path.isdir(os.path.join(hr_root, d))]
        
        for category in categories:
            hr_category_path = os.path.join(hr_root, category)
            lr_category_path = os.path.join(lr_scale_root, category)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(lr_category_path, exist_ok=True)
            
            # è·å–è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶
            image_files = []
            for file in os.listdir(hr_category_path):
                if os.path.splitext(file.lower())[1] in supported_formats:
                    image_files.append(file)
            
            # é™åˆ¶æ¯ä¸ªç±»åˆ«çš„å›¾åƒæ•°é‡
            if max_images_per_category and len(image_files) > max_images_per_category:
                image_files = image_files[:max_images_per_category]
            
            category_processed = 0
            category_failed = 0
            
            print(f"  ğŸ“ ç±»åˆ« {category}: {len(image_files)} å¼ å›¾åƒ")
            
            # ä½¿ç”¨è¿›åº¦æ¡å¤„ç†å›¾åƒ
            pbar = tqdm(image_files, desc=f"    å¤„ç† {category}", leave=False)
            
            for filename in pbar:
                hr_image_path = os.path.join(hr_category_path, filename)
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                base_name = os.path.splitext(filename)[0]
                lr_filename = f"{base_name}_upsampled_{scale}x.png"
                lr_output_path = os.path.join(lr_category_path, lr_filename)
                
                # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
                if os.path.exists(lr_output_path):
                    category_processed += 1
                    continue
                
                # å¤„ç†å›¾åƒ
                success, info = create_lr_hr_pairs(hr_image_path, lr_output_path, scale)
                
                if success:
                    category_processed += 1
                else:
                    category_failed += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'æˆåŠŸ': category_processed,
                    'å¤±è´¥': category_failed
                })
            
            scale_stats['categories'][category] = {
                'processed': category_processed,
                'failed': category_failed,
                'total': len(image_files)
            }
            
            scale_stats['processed'] += category_processed
            scale_stats['failed'] += category_failed
            
            print(f"    âœ… å®Œæˆ: {category_processed}/{len(image_files)} å¼ å›¾åƒ")
        
        processing_stats['categories'][f'scale_{scale}x'] = scale_stats
        processing_stats['total_processed'] += scale_stats['processed']
        processing_stats['total_failed'] += scale_stats['failed']
        
        print(f"  ğŸ“ˆ {scale}x é™è´¨å®Œæˆ: {scale_stats['processed']} æˆåŠŸ, {scale_stats['failed']} å¤±è´¥")
    
    # è®¡ç®—æ€»è€—æ—¶
    total_time = time.time() - processing_stats['start_time']
    processing_stats['total_time'] = total_time
    
    # ä¿å­˜å¤„ç†ç»Ÿè®¡
    stats_file = os.path.join(lr_root, 'processing_stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(processing_stats, f, indent=2, ensure_ascii=False)
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print(f"\nğŸ‰ è¶…åˆ†è¾¨ç‡æ•°æ®é¢„å¤„ç†å®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"âœ… æˆåŠŸå¤„ç†: {processing_stats['total_processed']} å¼ å›¾åƒ")
    print(f"âŒ å¤„ç†å¤±è´¥: {processing_stats['total_failed']} å¼ å›¾åƒ")
    print(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶: {stats_file}")
    print(f"ğŸ” æ•°æ®ç‰¹ç‚¹: RGBå½©è‰²å›¾åƒ, çœŸæ­£çš„è¶…åˆ†è¾¨ç‡ (256â†’1024)")
    
    # æ˜¾ç¤ºæ¯ä¸ªscaleçš„è¯¦ç»†ç»Ÿè®¡
    for scale in scales:
        scale_key = f'scale_{scale}x'
        if scale_key in processing_stats['categories']:
            stats = processing_stats['categories'][scale_key]
            print(f"\nğŸ“‹ {scale}xé™è´¨ç»Ÿè®¡:")
            for category, cat_stats in stats['categories'].items():
                success_rate = cat_stats['processed'] / cat_stats['total'] * 100 if cat_stats['total'] > 0 else 0
                print(f"  {category}: {cat_stats['processed']}/{cat_stats['total']} ({success_rate:.1f}%)")

def verify_generated_data(lr_root, scales):
    """éªŒè¯ç”Ÿæˆçš„æ•°æ®"""
    print(f"\nğŸ” éªŒè¯ç”Ÿæˆçš„æ•°æ®...")
    
    for scale in scales:
        lr_scale_root = os.path.join(lr_root, f'scale_{scale}x')
        if not os.path.exists(lr_scale_root):
            print(f"âŒ {scale}xæ•°æ®ç›®å½•ä¸å­˜åœ¨: {lr_scale_root}")
            continue
        
        total_images = 0
        categories = []
        
        for category in os.listdir(lr_scale_root):
            category_path = os.path.join(lr_scale_root, category)
            if os.path.isdir(category_path):
                category_count = len([f for f in os.listdir(category_path) 
                                    if f.endswith('.png')])
                categories.append((category, category_count))
                total_images += category_count
        
        print(f"âœ… {scale}xé™è´¨: {total_images} å¼ å›¾åƒ")
        for category, count in categories:
            print(f"  ğŸ“ {category}: {count} å¼ ")

def main():
    parser = argparse.ArgumentParser(description='è¶…åˆ†è¾¨ç‡æ•°æ®é¢„å¤„ç†å·¥å…· - RGBå½©è‰²å›¾åƒ')
    parser.add_argument('--hr_root', type=str, default='./OST',
                        help='é«˜åˆ†è¾¨ç‡å›¾åƒæ ¹ç›®å½•')
    parser.add_argument('--lr_root', type=str, default='./OST_LR',
                        help='ä½åˆ†è¾¨ç‡å›¾åƒè¾“å‡ºæ ¹ç›®å½•')
    parser.add_argument('--scales', type=int, nargs='+', default=[4],
                        help='é™è´¨å€æ•°åˆ—è¡¨ï¼Œé»˜è®¤: [4]')
    parser.add_argument('--max_per_category', type=int, default=None,
                        help='æ¯ä¸ªç±»åˆ«çš„æœ€å¤§å›¾åƒæ•°é‡')
    parser.add_argument('--verify', action='store_true',
                        help='ä»…éªŒè¯å·²ç”Ÿæˆçš„æ•°æ®')
    
    args = parser.parse_args()
    
    if args.verify:
        verify_generated_data(args.lr_root, args.scales)
    else:
        process_dataset(args.hr_root, args.lr_root, args.scales, args.max_per_category)
        verify_generated_data(args.lr_root, args.scales)

if __name__ == '__main__':
    print("=" * 70)
    print("ğŸ¨ è¶…åˆ†è¾¨ç‡æ•°æ®é¢„å¤„ç†å·¥å…·")
    print("ğŸ” åŠŸèƒ½: åˆ›å»ºRGBå½©è‰²å›¾åƒçš„è¶…åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®")
    print("ğŸ“ ä»»åŠ¡: é«˜åˆ†è¾¨ç‡(1024Ã—1024) â†’ ä½åˆ†è¾¨ç‡(256Ã—256)")
    print("=" * 70)
    
    import sys
    if len(sys.argv) == 1:
        print("ğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
        print("\n1. ç”Ÿæˆ4xé™è´¨æ•°æ®:")
        print("   python generate_lr_images_sr.py --scales 4")
        print("\n2. ç”Ÿæˆå¤šä¸ªå°ºåº¦çš„æ•°æ®:")
        print("   python generate_lr_images_sr.py --scales 2 4 8")
        print("\n3. é™åˆ¶æ¯ä¸ªç±»åˆ«çš„å›¾åƒæ•°é‡:")
        print("   python generate_lr_images_sr.py --scales 4 --max_per_category 1000")
        print("\n4. éªŒè¯å·²ç”Ÿæˆçš„æ•°æ®:")
        print("   python generate_lr_images_sr.py --verify --scales 4")
        print("\nğŸ“ å‚æ•°è¯´æ˜:")
        print("   --hr_root: é«˜åˆ†è¾¨ç‡å›¾åƒç›®å½• (é»˜è®¤: ./OST)")
        print("   --lr_root: ä½åˆ†è¾¨ç‡å›¾åƒè¾“å‡ºç›®å½• (é»˜è®¤: ./OST_LR)")
        print("   --scales: é™è´¨å€æ•° (é»˜è®¤: [4])")
        print("   --max_per_category: æ¯ç±»æœ€å¤§å›¾åƒæ•°")
        print("   --verify: ä»…éªŒè¯æ•°æ®")
        print(f"\nğŸ¯ ç‰¹ç‚¹: RGBå½©è‰², çœŸæ­£è¶…åˆ†è¾¨ç‡, 256Ã—256â†’1024Ã—1024")
        sys.exit(0)
    
    main()

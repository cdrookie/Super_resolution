# ========== ä½¿ç”¨é¢„ç”Ÿæˆä½åˆ†è¾¨ç‡å›¾åƒçš„è®­ç»ƒè„šæœ¬ ==========
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import random
import numpy as np
from PIL import Image
import torch.nn.functional as F
import json
import time

# å¯¼å…¥Real-ESRGANæ¨¡å‹
from basicsr.archs.rrdbnet_arch import RRDBNet

# ========== è®­ç»ƒé…ç½®å‚æ•° ==========
HR_DATA_ROOT = './OST'  # é«˜åˆ†è¾¨ç‡å›¾åƒç›®å½•
LR_DATA_ROOT = './OST_LR'  # é¢„ç”Ÿæˆçš„ä½åˆ†è¾¨ç‡å›¾åƒç›®å½•
SCALE_FACTOR = 4  # ä½¿ç”¨çš„é™è´¨å€æ•°
BATCH_SIZE = 2  # æ‰¹æ¬¡å¤§å°
NUM_EPOCHS = 15  # è®­ç»ƒè½®æ•°
LR = 1e-3  # å­¦ä¹ ç‡
NUM_WORKERS = 0  # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
MODEL_SAVE_PATH = './realESRGAN_from_pregenerated.pth'
VAL_SPLIT = 0.1  # éªŒè¯é›†æ¯”ä¾‹

# ========== æ¨¡å‹é…ç½® ==========
# å¯é€‰æ‹©ä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
MODEL_CONFIGS = {
    'ultra_lite': {
        'num_in_ch': 1, 'num_out_ch': 1,
        'num_feat': 24, 'num_block': 8, 'num_grow_ch': 12
    },
    'lite': {
        'num_in_ch': 1, 'num_out_ch': 1,
        'num_feat': 32, 'num_block': 12, 'num_grow_ch': 16
    },
    'standard': {
        'num_in_ch': 1, 'num_out_ch': 1,
        'num_feat': 48, 'num_block': 16, 'num_grow_ch': 24
    }
}

CURRENT_MODEL = 'ultra_lite'  # é€‰æ‹©æ¨¡å‹å¤æ‚åº¦

# ========== GPUä¼˜åŒ–è®¾ç½® ==========
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
torch.backends.cudnn.benchmark = True

# ========== é¢„ç”Ÿæˆå›¾åƒæ•°æ®é›†ç±» ==========
class PreGeneratedDataset(Dataset):
    """ä½¿ç”¨é¢„ç”Ÿæˆçš„ä½åˆ†è¾¨ç‡å›¾åƒçš„æ•°æ®é›†"""
    
    def __init__(self, hr_root, lr_root, scale_factor, image_pairs):
        self.hr_root = hr_root
        self.lr_root = lr_root
        self.scale_factor = scale_factor
        self.image_pairs = image_pairs
        
        print(f"æ•°æ®é›†åˆå§‹åŒ–: {len(image_pairs)} ä¸ªå›¾åƒå¯¹")
    
    def __len__(self):
        return len(self.image_pairs)
    
    def __getitem__(self, idx):
        hr_path, lr_upsampled_path = self.image_pairs[idx]
        
        try:
            # åŠ è½½é«˜åˆ†è¾¨ç‡å›¾åƒï¼ˆGround Truthï¼‰
            hr_img = self.load_image(hr_path)
            
            # åŠ è½½é¢„ç”Ÿæˆçš„ä½åˆ†è¾¨ç‡ä¸Šé‡‡æ ·å›¾åƒï¼ˆæ¨¡å‹è¾“å…¥ï¼‰
            lr_img = self.load_image(lr_upsampled_path)
            
            # è½¬æ¢ä¸ºtensor
            lr_tensor = torch.from_numpy(lr_img).unsqueeze(0).float()
            hr_tensor = torch.from_numpy(hr_img).unsqueeze(0).float()
            
            return lr_tensor, hr_tensor
        
        except Exception as e:
            print(f"âŒ åŠ è½½å›¾åƒå¯¹å¤±è´¥: {hr_path}, {lr_upsampled_path}, é”™è¯¯: {e}")
            # è¿”å›éšæœºæ•°æ®ä½œä¸ºfallback
            size = 512  # å‡è®¾çš„å›¾åƒå°ºå¯¸
            lr_tensor = torch.rand(1, size, size) * 0.8
            hr_tensor = torch.rand(1, size, size)
            return lr_tensor, hr_tensor
    
    def load_image(self, img_path):
        """åŠ è½½å•å¼ å›¾åƒå¹¶ç¡®ä¿å°ºå¯¸ä¸€è‡´"""
        img = Image.open(img_path).convert('L')
        img = np.array(img).astype('float32') / 255.0
        
        # ç¡®ä¿æ‰€æœ‰å›¾åƒéƒ½æ˜¯ç›¸åŒå°ºå¯¸ - ä¿®å¤tensorå°ºå¯¸ä¸åŒ¹é…é—®é¢˜
        target_size = 512  # å›ºå®šç›®æ ‡å°ºå¯¸
        if img.shape != (target_size, target_size):
            # ä½¿ç”¨PILè¿›è¡Œresizeä»¥é¿å…cv2ä¾èµ–é—®é¢˜
            img_pil = Image.fromarray((img * 255).astype('uint8'))
            img_pil_resized = img_pil.resize((target_size, target_size), Image.LANCZOS)
            img = np.array(img_pil_resized).astype('float32') / 255.0
        
        return img

# ========== æ•°æ®é…å¯¹å‡½æ•° ==========
def find_image_pairs(hr_root, lr_root, scale_factor):
    """
    æ‰¾åˆ°é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¯¹åº”çš„ä½åˆ†è¾¨ç‡å›¾åƒå¯¹
    Returns:
        image_pairs: [(hr_path, lr_upsampled_path), ...] çš„åˆ—è¡¨
    """
    image_pairs = []
    lr_scale_dir = os.path.join(lr_root, f'scale_{scale_factor}x')
    
    if not os.path.exists(lr_scale_dir):
        print(f"âŒ ä½åˆ†è¾¨ç‡ç›®å½•ä¸å­˜åœ¨: {lr_scale_dir}")
        return []
    
    print(f"ğŸ” æœç´¢å›¾åƒå¯¹...")
    print(f"  é«˜åˆ†è¾¨ç‡ç›®å½•: {hr_root}")
    print(f"  ä½åˆ†è¾¨ç‡ç›®å½•: {lr_scale_dir}")
    
    # éå†é«˜åˆ†è¾¨ç‡å›¾åƒç›®å½•
    for category in os.listdir(hr_root):
        hr_category_path = os.path.join(hr_root, category)
        lr_category_path = os.path.join(lr_scale_dir, category)
        
        if not os.path.isdir(hr_category_path) or not os.path.exists(lr_category_path):
            continue
        
        category_pairs = 0
        for hr_filename in os.listdir(hr_category_path):
            hr_path = os.path.join(hr_category_path, hr_filename)
            
            if not os.path.isfile(hr_path):
                continue
            
            # æ„å»ºå¯¹åº”çš„ä½åˆ†è¾¨ç‡ä¸Šé‡‡æ ·å›¾åƒè·¯å¾„
            base_name = os.path.splitext(hr_filename)[0]
            lr_upsampled_filename = f"{base_name}_upsampled_{scale_factor}x.png"
            lr_upsampled_path = os.path.join(lr_category_path, lr_upsampled_filename)
            
            if os.path.exists(lr_upsampled_path):
                image_pairs.append((hr_path, lr_upsampled_path))
                category_pairs += 1
        
        print(f"  ç±»åˆ« {category}: {category_pairs} ä¸ªå›¾åƒå¯¹")
    
    print(f"âœ… æ€»è®¡æ‰¾åˆ° {len(image_pairs)} ä¸ªæœ‰æ•ˆå›¾åƒå¯¹")
    return image_pairs

# ========== è®­ç»ƒå‡½æ•° ==========
def train_with_pregenerated_data():
    """ä½¿ç”¨é¢„ç”Ÿæˆæ•°æ®è¿›è¡Œè®­ç»ƒ"""
    print("=" * 60)
    print("ä½¿ç”¨é¢„ç”Ÿæˆä½åˆ†è¾¨ç‡å›¾åƒçš„Real-ESRGANè®­ç»ƒ")
    print("=" * 60)
    
    # æ£€æŸ¥ç›®å½•å­˜åœ¨æ€§
    if not os.path.exists(HR_DATA_ROOT):
        print(f"âŒ é«˜åˆ†è¾¨ç‡æ•°æ®ç›®å½•ä¸å­˜åœ¨: {HR_DATA_ROOT}")
        return
    
    if not os.path.exists(LR_DATA_ROOT):
        print(f"âŒ ä½åˆ†è¾¨ç‡æ•°æ®ç›®å½•ä¸å­˜åœ¨: {LR_DATA_ROOT}")
        print(f"è¯·å…ˆè¿è¡Œ generate_lr_images.py ç”Ÿæˆä½åˆ†è¾¨ç‡å›¾åƒ")
        return
    
    # æ‰¾åˆ°å›¾åƒå¯¹
    image_pairs = find_image_pairs(HR_DATA_ROOT, LR_DATA_ROOT, SCALE_FACTOR)
    if len(image_pairs) == 0:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å›¾åƒå¯¹!")
        return
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    random.shuffle(image_pairs)
    split_idx = int(len(image_pairs) * (1 - VAL_SPLIT))
    train_pairs = image_pairs[:split_idx]
    val_pairs = image_pairs[split_idx:]
    
    print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(train_pairs)} ä¸ªå›¾åƒå¯¹")
    print(f"  éªŒè¯é›†: {len(val_pairs)} ä¸ªå›¾åƒå¯¹")
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = PreGeneratedDataset(HR_DATA_ROOT, LR_DATA_ROOT, SCALE_FACTOR, train_pairs)
    val_dataset = PreGeneratedDataset(HR_DATA_ROOT, LR_DATA_ROOT, SCALE_FACTOR, val_pairs)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=NUM_WORKERS,
        pin_memory=torch.cuda.is_available()
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False, 
        num_workers=NUM_WORKERS,
        pin_memory=torch.cuda.is_available()
    )
    
    # è®¾ç½®è®¾å¤‡å’Œæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ è®­ç»ƒè®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        torch.cuda.empty_cache()
    
    # åˆ›å»ºæ¨¡å‹
    model_config = MODEL_CONFIGS[CURRENT_MODEL]
    model = RRDBNet(**model_config)
    model = model.to(device)
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ§  æ¨¡å‹é…ç½®: {CURRENT_MODEL}")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {total_params:,} ({total_params/1e6:.2f}M)")
    print(f"âš™ï¸  æ¨¡å‹è¯¦æƒ…: {model_config}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.L1Loss()
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=LR, epochs=NUM_EPOCHS, 
        steps_per_epoch=len(train_loader), pct_start=0.1
    )
    
    # è®­ç»ƒé…ç½®ä¿¡æ¯
    print(f"\nğŸš€ è®­ç»ƒé…ç½®:")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
    print(f"  å­¦ä¹ ç‡: {LR}")
    print(f"  é™è´¨å€æ•°: {SCALE_FACTOR}x")
    
    # å¼€å§‹è®­ç»ƒ
    best_val_loss = float('inf')
    training_start_time = time.time()
    
    training_log = {
        'config': model_config,
        'epochs': [],
        'best_val_loss': float('inf'),
        'total_time': 0
    }
    
    for epoch in range(NUM_EPOCHS):
        epoch_start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} - è®­ç»ƒ')
        
        for batch_idx, (lr_img, hr_img) in enumerate(train_pbar):
            lr_img = lr_img.to(device, non_blocking=True)
            hr_img = hr_img.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            pred = model(lr_img)
            
            # ç¡®ä¿é¢„æµ‹ç»“æœä¸ç›®æ ‡å°ºå¯¸ä¸€è‡´
            if pred.shape[-2:] != hr_img.shape[-2:]:
                pred = F.interpolate(pred, size=hr_img.shape[-2:], mode='bilinear', align_corners=False)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(pred, hr_img)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            batch_loss = loss.item()
            train_loss += batch_loss
            
            # æ›´æ–°è¿›åº¦æ¡
            train_pbar.set_postfix({
                'Loss': f'{batch_loss:.4f}',
                'LR': f'{optimizer.param_groups[0]["lr"]:.2e}'
            })
            
            # å®šæœŸæ¸…ç†GPUå†…å­˜
            if batch_idx % 50 == 0 and torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        train_loss /= len(train_loader)
        
        # éªŒè¯é˜¶æ®µï¼ˆæ¯2ä¸ªepochä¸€æ¬¡ï¼‰
        val_loss = 0
        if (epoch + 1) % 2 == 0 or epoch == NUM_EPOCHS - 1:
            model.eval()
            with torch.no_grad():
                val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} - éªŒè¯', leave=False)
                for lr_img, hr_img in val_pbar:
                    lr_img = lr_img.to(device, non_blocking=True)
                    hr_img = hr_img.to(device, non_blocking=True)
                    
                    pred = model(lr_img)
                    if pred.shape[-2:] != hr_img.shape[-2:]:
                        pred = F.interpolate(pred, size=hr_img.shape[-2:], mode='bilinear', align_corners=False)
                    
                    loss = criterion(pred, hr_img)
                    val_loss += loss.item()
                    
                    val_pbar.set_postfix({'Val Loss': f'{loss.item():.4f}'})
            
            val_loss /= len(val_loader)
            
            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model.state_dict(), MODEL_SAVE_PATH)
                print(f'[ä¿å­˜] æœ€ä½³æ¨¡å‹ Epoch {epoch+1} | éªŒè¯æŸå¤±={val_loss:.4f}')
        
        epoch_time = time.time() - epoch_start_time
        
        # è®°å½•è®­ç»ƒæ—¥å¿—
        epoch_log = {
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'time': epoch_time,
            'lr': optimizer.param_groups[0]['lr']
        }
        training_log['epochs'].append(epoch_log)
        
        print(f'Epoch {epoch+1}: è®­ç»ƒæŸå¤±={train_loss:.4f}, éªŒè¯æŸå¤±={val_loss:.4f}, æ—¶é—´={epoch_time:.1f}s')
        
        # GPUå†…å­˜çŠ¶æ€
        if torch.cuda.is_available() and (epoch + 1) % 3 == 0:
            allocated = torch.cuda.memory_allocated() / 1024**3
            print(f'GPUå†…å­˜: {allocated:.2f}GB')
    
    # è®­ç»ƒå®Œæˆç»Ÿè®¡
    total_time = time.time() - training_start_time
    training_log['total_time'] = total_time
    training_log['best_val_loss'] = best_val_loss
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    log_file = MODEL_SAVE_PATH.replace('.pth', '_training_log.json')
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(training_log, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"âš¡ å¹³å‡æ¯è½®: {total_time/NUM_EPOCHS:.1f} ç§’")
    print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {MODEL_SAVE_PATH}")
    print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—: {log_file}")

# ========== ä¸»å‡½æ•° ==========
if __name__ == '__main__':
    print("æ£€æŸ¥é¢„ç”Ÿæˆæ•°æ®...")
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„ç”Ÿæˆçš„æ•°æ®
    lr_scale_dir = os.path.join(LR_DATA_ROOT, f'scale_{SCALE_FACTOR}x')
    if not os.path.exists(lr_scale_dir):
        print(f"âŒ æœªæ‰¾åˆ°é¢„ç”Ÿæˆçš„ä½åˆ†è¾¨ç‡æ•°æ®: {lr_scale_dir}")
        print("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆä½åˆ†è¾¨ç‡å›¾åƒ:")
        print(f"python generate_lr_images.py --scales {SCALE_FACTOR}")
        exit(1)
    
    # å¼€å§‹è®­ç»ƒ
    train_with_pregenerated_data()
